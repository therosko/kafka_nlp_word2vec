{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine files saved from the Twitter Consumer\n",
    "After creating multiple files, saved through the Twitter consumer, we combine all of them into one and clean out the emptry rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the names of the folders are unpredictable, we make sure we loop through all folders and all files within\n",
    "filepaths = glob.glob(\"data/elon-*/part-0*\")\n",
    "filepaths2 = glob.glob(\"data/elon-*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/collect.txt\",\"a\") as writefile:\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath,\"r\") as readfile:\n",
    "            data = readfile.read()\n",
    "            writefile.write(data)\n",
    "        os.remove(filepath)\n",
    "for filepath2 in filepaths2:\n",
    "    shutil.rmtree(filepath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear empty rows\n",
    "with open('./data/collect.txt') as infile, open('data/output.txt', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if not line.strip(): continue  # skip the empty line\n",
    "        outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark \n",
    "import findspark\n",
    "\n",
    "# Initialize and provide path\n",
    "findspark.init(\"/usr/share/spark/spark-2.3.2-bin-hadoop2.7/\")\n",
    "\n",
    "# Or use this alternative\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data for Natural Language Processing\n",
    "Here we read in the one merged file with all Tweets, creating a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['RT @MariyaAlexander: Howard Schultz is basically a less likable Jill Stein, a dumber Gary Johnson, and a more embarrassing Elon Musk. '],\n",
       " ['RT @TheWilsonCenter: Congratulations to @wapodavenport for his book \"The Space Barons: Elon Musk, Jeff Bezos, and the Quest to Colonize the… '],\n",
       " ['@thomas_violence Hahahahahahahahahhahahaha you fucking idiot , I mean lots come to mind but let’s put a well known… https://t.co/3NMFBY2pzr '],\n",
       " ['RT @Tesla_Bear: (1)'],\n",
       " ['MOST important part of the conference call: Elon Musk desperately avoiding to give ANY detail about demand in Europe a… ']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "data_rdd = sc.textFile('data/output.txt') \n",
    "parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "\n",
    "# Filter bad rows out\n",
    "#garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "\n",
    "#typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "#typed_rdd.take(5)\n",
    "parts_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                text|\n",
      "+--------------------+\n",
      "|RT @MariyaAlexand...|\n",
      "|RT @TheWilsonCent...|\n",
      "|@thomas_violence ...|\n",
      "| RT @Tesla_Bear: (1)|\n",
      "|MOST important pa...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = sqlContext.createDataFrame(parts_rdd, [\"text\"])\n",
    "\n",
    "# data_df.printSchema()\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "After creating the data frame, we move on to the natural language processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Note: Uncomment the two lines below before the first execution of the code:\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/student/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preproc as pp\n",
    "\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "\n",
    "'''Use langid module to classify the language to make sure we are applying the correct cleanup actions for English\n",
    "https://github.com/saffsd/langid.py '''\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "\n",
    "'''Stop words usually refer to the most common words in a language, there is no single universal list of stop words  \n",
    "used by all natural language processing tools. \n",
    "Reduces Dimensionality removes stop words of a single Tweets (cleaned_str/row/document)'''\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "\n",
    "'''catch-all to remove other 'words' that I felt didn't add a lot of value\n",
    "Reduces Dimensionality, gets rid of a lot of unique urls'''\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "\n",
    "'''Process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech\n",
    "tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The\n",
    "collection of tags used for a particular task is known as a tagset. Our emphasis in this chapter is on exploiting\n",
    "tags, and tagging text automatically. http://www.nltk.org/book/ch05.html'''\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "\n",
    "'''Tweets are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, \n",
    "there are families of derivationally related words with similar meanings, such as democracy, democratic, and \n",
    "democratization. In many situations, it seems as if it would be useful for a search for one of these words to return \n",
    "documents that contain another word in the set.\n",
    "Reduces Dimensionality and boosts numerical measures like TFIDF\n",
    "http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "lemmatization of a single Tweets (cleaned_str/row/document)'''\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "\n",
    "'''check to see if a row only contains whitespace '''\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|           stop_text|\n",
      "+--------------------+--------------------+\n",
      "|RT @MariyaAlexand...|RT @MariyaAlexand...|\n",
      "|RT @TheWilsonCent...|RT @TheWilsonCent...|\n",
      "|@thomas_violence ...|@thomas_violence ...|\n",
      "| RT @Tesla_Bear: (1)| RT @Tesla_Bear: (1)|\n",
      "|MOST important pa...|MOST important pa...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''remove stop words to reduce dimensionality\n",
    "list of stop words: https://gist.github.com/sebleier/554280'''\n",
    "rm_stops_df = data_df.withColumn(\"stop_text\", remove_stops_udf(data_df[\"text\"]))\n",
    "rm_stops_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                text|           stop_text|           feat_text|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|RT @MariyaAlexand...|RT @MariyaAlexand...|  howard schultz ...|\n",
      "|RT @TheWilsonCent...|RT @TheWilsonCent...|  congratulations...|\n",
      "|@thomas_violence ...|@thomas_violence ...|hahahahahahahahah...|\n",
      "| RT @Tesla_Bear: (1)| RT @Tesla_Bear: (1)|                    |\n",
      "|MOST important pa...|MOST important pa...|most important pa...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df = rm_stops_df.withColumn(\"feat_text\", remove_features_udf(rm_stops_df[\"stop_text\"]))\n",
    "rm_features_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|           stop_text|           feat_text|         tagged_text|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|RT @MariyaAlexand...|RT @MariyaAlexand...|  howard schultz ...| howard schultz l...|\n",
      "|RT @TheWilsonCent...|RT @TheWilsonCent...|  congratulations...| congratulations ...|\n",
      "|@thomas_violence ...|@thomas_violence ...|hahahahahahahahah...| hahahahahahahaha...|\n",
      "| RT @Tesla_Bear: (1)| RT @Tesla_Bear: (1)|                    |                    |\n",
      "|MOST important pa...|MOST important pa...|most important pa...| important part c...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df = rm_features_df.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df[\"feat_text\"]))\n",
    "tagged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|           stop_text|           feat_text|         tagged_text|           lemm_text|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|RT @MariyaAlexand...|RT @MariyaAlexand...|  howard schultz ...| howard schultz l...|howard schultz li...|\n",
      "|RT @TheWilsonCent...|RT @TheWilsonCent...|  congratulations...| congratulations ...|congratulation bo...|\n",
      "|@thomas_violence ...|@thomas_violence ...|hahahahahahahahah...| hahahahahahahaha...|hahahahahahahahah...|\n",
      "| RT @Tesla_Bear: (1)| RT @Tesla_Bear: (1)|                    |                    |                    |\n",
      "|MOST important pa...|MOST important pa...|most important pa...| important part c...|important part co...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df = tagged_df.withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))\n",
    "lemm_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|                text|           stop_text|           feat_text|         tagged_text|           lemm_text|is_blank|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "|RT @MariyaAlexand...|RT @MariyaAlexand...|  howard schultz ...| howard schultz l...|howard schultz li...|   False|\n",
      "|RT @TheWilsonCent...|RT @TheWilsonCent...|  congratulations...| congratulations ...|congratulation bo...|   False|\n",
      "|@thomas_violence ...|@thomas_violence ...|hahahahahahahahah...| hahahahahahahaha...|hahahahahahahahah...|   False|\n",
      "| RT @Tesla_Bear: (1)| RT @Tesla_Bear: (1)|                    |                    |                    |   False|\n",
      "|MOST important pa...|MOST important pa...|most important pa...| important part c...|important part co...|   False|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove all rows containing only blank spaces - can be skipped as it was cleaned before?\n",
    "check_blanks_df = lemm_df.withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")\n",
    "\n",
    "no_blanks_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           lemm_text|\n",
      "+--------------------+\n",
      "|howard schultz li...|\n",
      "|congratulation bo...|\n",
      "|hahahahahahahahah...|\n",
      "|                    |\n",
      "|important part co...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_set = no_blanks_df.select(no_blanks_df['lemm_text'])\n",
    "\n",
    "data_set.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' https://spark.apache.org/docs/1.6.2/ml-features.html\n",
    "TF: HashingTF is a Transformer which takes sets of terms and converts those sets into fixed-length feature vectors.\n",
    "In text processing, a “set of terms” might be a bag of words. The algorithm combines Term Frequency (TF) counts with \n",
    "the hashing trick for dimensionality reduction.'''\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"lemm_text\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(data_set)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|           lemm_text|               words|         rawFeatures|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|howard schultz li...|[howard, schultz,...|(20,[0,3,7,8,9,15...|\n",
      "|congratulation bo...|[congratulation, ...|(20,[0,5,6,7,13,1...|\n",
      "|hahahahahahahahah...|[hahahahahahahaha...|(20,[4,9,12,15,18...|\n",
      "|                    |                  []|     (20,[12],[1.0])|\n",
      "|important part co...|[important, part,...|(20,[0,1,3,6,7,9,...|\n",
      "|elon musk share i...|[elon, musk, shar...|(20,[0,3,13,16,17...|\n",
      "|                    |                  []|     (20,[12],[1.0])|\n",
      "|           bait elon|        [bait, elon]|(20,[0,19],[1.0,1...|\n",
      "|washington post p...|[washington, post...|(20,[0,4,6,11,12,...|\n",
      "|  elon musk fly mile|[elon, musk, fly,...|(20,[0,15,16,18],...|\n",
      "|         elon review|      [elon, review]|(20,[0,4],[1.0,1.0])|\n",
      "|dank meme bot mem...|[dank, meme, bot,...|(20,[1,5,7,14],[1...|\n",
      "|heard real time f...|[heard, real, tim...|(20,[0,5,7,9,11,1...|\n",
      "|         elon review|      [elon, review]|(20,[0,4],[1.0,1.0])|\n",
      "|lol rofl lmao lmf...|[lol, rofl, lmao,...|(20,[1,3,6,7,10,1...|\n",
      "|omg elon boi tony...|[omg, elon, boi, ...|(20,[0,4,6,10,12,...|\n",
      "| favorite tweet time|[favorite, tweet,...|(20,[6,11,17],[1....|\n",
      "|elon musk tell in...|[elon, musk, tell...|(20,[0,4,6,7,10,1...|\n",
      "|elon musk tell in...|[elon, musk, tell...|(20,[0,4,6,7,10,1...|\n",
      "|stephen king own ...|[stephen, king, o...|(20,[0,8,9,12,14,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurizedData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Note: the execution here takes a bit longer that the rest of the cells\n",
    "https://stackoverflow.com/questions/38610559/convert-spark-dataframe-column-to-python-list '''\n",
    "tweets = featurizedData.select(\"words\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and check results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells were kindly provided to us by Kavita Ganesan. The GitHub repository we downloaded the needed code is: https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec . In her blog post \"Gensim Word2Vec Tutorial - Full Working Example\"( http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XFHO8M9KhZJ ) she explains the steps and provides a link to her jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-31 12:02:18,494 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-01-31 12:02:18,495 : INFO : collecting all words and their counts\n",
      "2019-01-31 12:02:18,496 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-31 12:02:18,498 : INFO : collected 1246 word types from a corpus of 5582 raw words and 833 sentences\n",
      "2019-01-31 12:02:18,499 : INFO : Loading a fresh vocabulary\n",
      "2019-01-31 12:02:18,501 : INFO : effective_min_count=2 retains 565 unique words (45% of original 1246, drops 681)\n",
      "2019-01-31 12:02:18,502 : INFO : effective_min_count=2 leaves 4901 word corpus (87% of original 5582, drops 681)\n",
      "2019-01-31 12:02:18,506 : INFO : deleting the raw counts dictionary of 1246 items\n",
      "2019-01-31 12:02:18,507 : INFO : sample=0.001 downsamples 81 most-common words\n",
      "2019-01-31 12:02:18,508 : INFO : downsampling leaves estimated 3335 word corpus (68.1% of prior 4901)\n",
      "2019-01-31 12:02:18,510 : INFO : estimated required memory for 565 words and 150 dimensions: 960500 bytes\n",
      "2019-01-31 12:02:18,511 : INFO : resetting layer weights\n",
      "2019-01-31 12:02:18,531 : INFO : training model with 10 workers on 565 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-31 12:02:18,538 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,539 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,540 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,541 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,541 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,542 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,563 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,564 : INFO : EPOCH - 1 : training on 5582 raw words (3260 effective words) took 0.0s, 125878 effective words/s\n",
      "2019-01-31 12:02:18,578 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,579 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,580 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,580 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,581 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,582 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,582 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,583 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,584 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,591 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,591 : INFO : EPOCH - 2 : training on 5582 raw words (3357 effective words) took 0.0s, 208477 effective words/s\n",
      "2019-01-31 12:02:18,601 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,602 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,603 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,604 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,604 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,605 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,605 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,606 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,615 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,663 : INFO : EPOCH - 3 : training on 5582 raw words (3375 effective words) took 0.1s, 52403 effective words/s\n",
      "2019-01-31 12:02:18,674 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,675 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,675 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,676 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,677 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,677 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,678 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,679 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,679 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,687 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,687 : INFO : EPOCH - 4 : training on 5582 raw words (3343 effective words) took 0.0s, 201057 effective words/s\n",
      "2019-01-31 12:02:18,697 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,699 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,699 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,764 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,764 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,765 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,766 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,767 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,773 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,774 : INFO : EPOCH - 5 : training on 5582 raw words (3352 effective words) took 0.1s, 42273 effective words/s\n",
      "2019-01-31 12:02:18,774 : INFO : training on a 27910 raw words (16687 effective words) took 0.2s, 68727 effective words/s\n",
      "2019-01-31 12:02:18,775 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2019-01-31 12:02:18,776 : INFO : training model with 10 workers on 565 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-31 12:02:18,784 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,785 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,787 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,788 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,789 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,790 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,791 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,792 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,794 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,795 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,796 : INFO : EPOCH - 1 : training on 5582 raw words (3365 effective words) took 0.0s, 287924 effective words/s\n",
      "2019-01-31 12:02:18,870 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,872 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,873 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,874 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-31 12:02:18,875 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,876 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,877 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,878 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,879 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,880 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,881 : INFO : EPOCH - 2 : training on 5582 raw words (3279 effective words) took 0.0s, 264229 effective words/s\n",
      "2019-01-31 12:02:18,893 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,895 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,896 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,897 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,898 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,898 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,899 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,900 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,900 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,908 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,964 : INFO : EPOCH - 3 : training on 5582 raw words (3335 effective words) took 0.1s, 47538 effective words/s\n",
      "2019-01-31 12:02:18,974 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,976 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,976 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,977 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:18,978 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:18,979 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:18,980 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:18,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:18,981 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:18,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:18,985 : INFO : EPOCH - 4 : training on 5582 raw words (3348 effective words) took 0.0s, 307297 effective words/s\n",
      "2019-01-31 12:02:18,997 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:18,997 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:18,998 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:18,999 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:19,000 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:19,001 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:19,001 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:19,002 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:19,003 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:19,005 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:19,006 : INFO : EPOCH - 5 : training on 5582 raw words (3370 effective words) took 0.0s, 287986 effective words/s\n",
      "2019-01-31 12:02:19,069 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:19,070 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:19,071 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:19,071 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:19,072 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:19,073 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:19,073 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:19,075 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:19,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:19,078 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:19,079 : INFO : EPOCH - 6 : training on 5582 raw words (3340 effective words) took 0.0s, 333571 effective words/s\n",
      "2019-01-31 12:02:19,092 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:19,093 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:19,094 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:19,094 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:19,095 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:19,096 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:19,096 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:19,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:19,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:19,101 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:19,102 : INFO : EPOCH - 7 : training on 5582 raw words (3341 effective words) took 0.0s, 292249 effective words/s\n",
      "2019-01-31 12:02:19,172 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:19,174 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:19,175 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:19,175 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:19,176 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:19,177 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:19,178 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:19,179 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:19,179 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:19,187 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:19,187 : INFO : EPOCH - 8 : training on 5582 raw words (3328 effective words) took 0.0s, 187837 effective words/s\n",
      "2019-01-31 12:02:19,196 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:19,197 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:19,198 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-01-31 12:02:19,199 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:19,200 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:19,200 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:19,201 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:19,264 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:19,264 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:19,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:19,265 : INFO : EPOCH - 9 : training on 5582 raw words (3344 effective words) took 0.1s, 46658 effective words/s\n",
      "2019-01-31 12:02:19,276 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-01-31 12:02:19,277 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-01-31 12:02:19,278 : INFO : worker thread finished; awaiting finish of 7 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-31 12:02:19,278 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-01-31 12:02:19,279 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-01-31 12:02:19,280 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-01-31 12:02:19,281 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-31 12:02:19,281 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-31 12:02:19,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-31 12:02:19,285 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-31 12:02:19,286 : INFO : EPOCH - 10 : training on 5582 raw words (3387 effective words) took 0.0s, 329413 effective words/s\n",
      "2019-01-31 12:02:19,287 : INFO : training on a 55820 raw words (33437 effective words) took 0.5s, 65520 effective words/s\n",
      "2019-01-31 12:02:19,287 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(33437, 55820)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read \n",
    "in the previous step. So, we are essentially passing on a list of lists. Where each list within the main list \n",
    "contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. \n",
    "And by vocabulary, I mean a set of unique words.'''\n",
    "\n",
    "model = gensim.models.Word2Vec (tweets, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(tweets,total_examples=len(tweets),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('musk', 0.9999439716339111),\n",
       " ('good', 0.9999069571495056),\n",
       " ('tesla', 0.9999029636383057),\n",
       " ('make', 0.9998981952667236),\n",
       " ('tsla', 0.9998976588249207),\n",
       " ('world', 0.9998967051506042),\n",
       " ('read', 0.999894917011261),\n",
       " ('thing', 0.9998947381973267),\n",
       " ('profit', 0.9998939037322998),\n",
       " ('think', 0.9998922348022461)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check whichever word you want\n",
    "w1 = \"elon\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
